{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GMF\n",
    "#The generalized matrix factorization introduced in Xiangnan He, Neural Collaborative Filtering\n",
    "#The GMF is one of the pretrained models we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance Reference\n",
    "#8 0.64 0.36\n",
    "#16 0.69 0.40\n",
    "#32 0.71 0.43\n",
    "#64 0.7 0.43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import heapq\n",
    "import time\n",
    "from math import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "with open('train_data.txt','r') as infile:\n",
    "    for l in infile:\n",
    "        l.rstrip('\\n')\n",
    "        lint = [int(x) for x in l.split(',')]\n",
    "        train_data.append(lint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_of_user = [[] for i in range(6040)]\n",
    "for l in train_data:\n",
    "    u = l[0]\n",
    "    item = l[1]\n",
    "    item_of_user[u].append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "with open('test_data.txt','r') as infile:\n",
    "    for l in infile:\n",
    "        l.rstrip('\\n')\n",
    "        lint = [int(x) for x in l.split(',')]\n",
    "        test_data.append(lint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_pair = [[] for i in range(6040)]\n",
    "with open('negative_pairs.txt','r') as infile:\n",
    "    for l in infile:\n",
    "        l.rstrip('\\n')\n",
    "        u,i = (int(x) for x in l.split(','))\n",
    "        negative_pair[u].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_user = 6040\n",
    "num_item = 3952"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For this task, we are performing a regression-like task, therefore the batch contains users, items and labels. \n",
    "def get_data(negative_num):\n",
    "    batch_user, batch_item, batch_label = [], [], []\n",
    "    for u in range(num_user):\n",
    "        for it in item_of_user[u]:\n",
    "            batch_user.append(u)\n",
    "            batch_item.append(it)\n",
    "            batch_label.append(1)\n",
    "            sampled_negative = 0\n",
    "            while sampled_negative <negative_num:\n",
    "                j = np.random.randint(num_item)\n",
    "                if j in item_of_user[u]:\n",
    "                    continue\n",
    "                else:\n",
    "                    batch_user.append(u)\n",
    "                    batch_item.append(j)\n",
    "                    batch_label.append(0)\n",
    "                    sampled_negative += 1\n",
    "    \n",
    "    return np.array(batch_user), np.array(batch_item), np.array(batch_label)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size = 32\n",
    "batch_size = 256\n",
    "learning_rate = 0.001\n",
    "regularization_rate = 1e-20\n",
    "max_epochs = 20\n",
    "negative_ratio = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "gamma_u = tf.get_variable(name = 'GMF_latent_u', shape = [num_user, latent_size], initializer = tf.random_normal_initializer(stddev = 0.02))\n",
    "gamma_i = tf.get_variable(name = 'GMF_latent_i', shape = [num_item, latent_size], initializer = tf.random_normal_initializer(stddev = 0.02))\n",
    "w_gmf = tf.get_variable(name = \"GMF_weight\", shape = [latent_size, 1], initializer = tf.random_normal_initializer(stddev = 0.02))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = tf.placeholder(tf.int32, [None])\n",
    "I = tf.placeholder(tf.int32, [None])\n",
    "label = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "batch_latent_u = tf.nn.embedding_lookup(gamma_u, U)\n",
    "batch_latent_i = tf.nn.embedding_lookup(gamma_i, I)\n",
    "\n",
    "merged_ui = tf.multiply(batch_latent_u, batch_latent_i)\n",
    "#print(merged_ui.shape)\n",
    "\n",
    "score = tf.reshape(tf.matmul(merged_ui, w_gmf), [-1])\n",
    "#print(score.shape)\n",
    "prediction = tf.nn.sigmoid(score)\n",
    "regularizer = tf.contrib.layers.l2_regularizer(regularization_rate)\n",
    "\n",
    "reg_term = regularizer(gamma_u) + regularizer(gamma_i)\n",
    "loss = reg_term + tf.losses.log_loss(labels = tf.cast(label, tf.float32), predictions = tf.reshape(prediction, [-1]))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "grad = tf.gradients(loss, [gamma_u, gamma_i, w_gmf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[54.1 s] After 0 epochs, loss on batch is 0.278.\n",
      "[57.7 s] After 0 epochs, hit@10 = 0.573, NDCG@10 = 0.319.\n",
      "[52.9 s] After 1 epochs, loss on batch is 0.261.\n",
      "[56.4 s] After 1 epochs, hit@10 = 0.630, NDCG@10 = 0.364.\n",
      "[52.3 s] After 2 epochs, loss on batch is 0.246.\n",
      "[55.9 s] After 2 epochs, hit@10 = 0.663, NDCG@10 = 0.385.\n",
      "[52.7 s] After 3 epochs, loss on batch is 0.244.\n",
      "[56.3 s] After 3 epochs, hit@10 = 0.676, NDCG@10 = 0.397.\n",
      "[51.7 s] After 4 epochs, loss on batch is 0.192.\n",
      "[55.2 s] After 4 epochs, hit@10 = 0.686, NDCG@10 = 0.406.\n",
      "[51.9 s] After 5 epochs, loss on batch is 0.227.\n",
      "[55.9 s] After 5 epochs, hit@10 = 0.691, NDCG@10 = 0.411.\n",
      "[53.0 s] After 6 epochs, loss on batch is 0.187.\n",
      "[56.6 s] After 6 epochs, hit@10 = 0.696, NDCG@10 = 0.416.\n",
      "[54.7 s] After 7 epochs, loss on batch is 0.227.\n",
      "[58.2 s] After 7 epochs, hit@10 = 0.694, NDCG@10 = 0.413.\n",
      "[53.0 s] After 8 epochs, loss on batch is 0.207.\n",
      "[56.7 s] After 8 epochs, hit@10 = 0.696, NDCG@10 = 0.417.\n",
      "[52.1 s] After 9 epochs, loss on batch is 0.203.\n",
      "[55.8 s] After 9 epochs, hit@10 = 0.701, NDCG@10 = 0.420.\n",
      "[52.4 s] After 10 epochs, loss on batch is 0.200.\n",
      "[56.0 s] After 10 epochs, hit@10 = 0.703, NDCG@10 = 0.423.\n",
      "[52.0 s] After 11 epochs, loss on batch is 0.207.\n",
      "[55.7 s] After 11 epochs, hit@10 = 0.701, NDCG@10 = 0.421.\n",
      "[52.5 s] After 12 epochs, loss on batch is 0.234.\n",
      "[56.2 s] After 12 epochs, hit@10 = 0.707, NDCG@10 = 0.424.\n",
      "[51.9 s] After 13 epochs, loss on batch is 0.212.\n",
      "[55.9 s] After 13 epochs, hit@10 = 0.703, NDCG@10 = 0.421.\n",
      "[54.2 s] After 14 epochs, loss on batch is 0.233.\n",
      "[57.8 s] After 14 epochs, hit@10 = 0.708, NDCG@10 = 0.422.\n",
      "[51.9 s] After 15 epochs, loss on batch is 0.171.\n",
      "[55.4 s] After 15 epochs, hit@10 = 0.708, NDCG@10 = 0.425.\n",
      "[52.0 s] After 16 epochs, loss on batch is 0.269.\n",
      "[55.7 s] After 16 epochs, hit@10 = 0.706, NDCG@10 = 0.427.\n",
      "[52.0 s] After 17 epochs, loss on batch is 0.209.\n",
      "[55.7 s] After 17 epochs, hit@10 = 0.705, NDCG@10 = 0.423.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-99dad7da2329>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0meu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mei\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-31abb5df8a24>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(negative_num)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msampled_negative\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mnegative_num\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_item\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitem_of_user\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    best_hr = 0\n",
    "    for ep in range(max_epochs):\n",
    "        t1 = time.time()\n",
    "        eu,ei,el = get_data(negative_ratio)\n",
    "        for j in range(len(eu)//batch_size):\n",
    "            sample = np.random.randint(len(eu), size = batch_size)\n",
    "            #print(sample)\n",
    "            bu = eu[sample]\n",
    "            bi = ei[sample]\n",
    "            bl = el[sample]\n",
    "            #print(bu)\n",
    "            #print(bi)\n",
    "            #print(bl)\n",
    "            loss_value, grad_value ,_ = sess.run([loss,grad, optimizer], feed_dict = {U:bu, I:bi, label:bl}) \n",
    "            #print(grad_value)\n",
    "    \n",
    "        t2 = time.time()\n",
    "        print(\"[%.1f s] After %d epochs, loss on batch is %.3f.\"%(t2-t1, ep, loss_value))\n",
    "        \n",
    "        hits = 0\n",
    "        tNDCG = 0\n",
    "        for u in range(num_user):\n",
    "            map_item_rating = {}\n",
    "            maxScore = sess.run(score, feed_dict = {U:np.array([u]), I:np.array([test_data[u][1]])})\n",
    "            negative_i = negative_pair[u]\n",
    "            negative_u = [u for m in range(100)]\n",
    "            negative_score = sess.run(score, feed_dict = {U:negative_u, I:negative_i})\n",
    "            map_item_rating[test_data[u][1]] = maxScore\n",
    "            for k in range(100):\n",
    "                map_item_rating[negative_i[k]] = negative_score[k]\n",
    "            ranklist = heapq.nlargest(10, map_item_rating, key = map_item_rating.get)\n",
    "            if test_data[u][1] in ranklist:\n",
    "                hits +=1\n",
    "                idx = ranklist.index(test_data[u][1])\n",
    "                tNDCG += log(2)/log(idx+2)\n",
    "        \n",
    "        hr = hits/6040\n",
    "        NDCG = tNDCG/6040\n",
    "        print(\"[%.1f s] After %d epochs, hit@10 = %.3f, NDCG@10 = %.3f.\"%(time.time()-t1, ep, hits/6040, tNDCG/6040))\n",
    "\n",
    "\n",
    "        if hr > best_hr:\n",
    "            best_hr = hr\n",
    "            saver = tf.train.Saver()\n",
    "            saver.save(sess, \"GMFModel/model.ckpt\", global_step = ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
